subsampling:
    CutDetectionSubsampler:
        args:
            # leave as is.
            cut_detection_mode: "all"
            framerates: null
            # see https://www.scenedetect.com/docs/latest/api/detectors.html#module-scenedetect.detectors
            threshold: 5
            # XXX: Normalized to work in the same way with videos of different fps.
            # XXX: So it behaves the same way for e.g., 30 and 60fps. Base fps is 30.
            # XXX: Thus: min_scene_len=30 --> min. 1 second, min_scene_len=60 --> min. 2 seconds, ..
            min_scene_len: 30
            algorithm: "adaptive"

reading:
    yt_args:
        download_size: 720 # 360?
        download_audio_rate: 44100
        video_codec: "avc1"
        fps: 20
        yt_metadata_args:
            writesubtitles: 'all'
            subtitleslangs: ['en']
            writeautomaticsub: True
            get_info: True
    timeout: 60
    sampler: null

storage:
    number_sample_per_shard: 20  # reduce this if you're not downloading thousands of videos to allow for enough parallelsim
    oom_shard_count: 10
    captions_are_subtitles: False

distribution:
    processes_count: 32
    thread_count: 64
    subjob_size: 10000 # seems not to be used (only for pyspark)
    distributor: "slurm"
    distributor_args:
        partition: "normal"
        n_nodes: 1
        account: es_cott  # FIXME: set this to your slurm account
        cache_path: "./slurm_cache"
        cpus_per_task: 32 # 1 node has 128 cores
        tasks_per_node: 1
        job_name: "v2d-cut"
        gpus_per_node: 0
        # gpu_mem: "24g"